{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "(http://deeplearning.net/tutorial/)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Formalization of Learning\n",
    "\n",
    "Given:\n",
    "* training examples $D=\\{z_1, z_2, ..., z_n \\}$ where $z_i$ is sampled from an **unkown** process $P(Z)$\n",
    "* loss function $L(f,Z)$ with arguments:\n",
    "    * decision function $f$\n",
    "    * example $z$\n",
    "    * return scalar $\\in I\\!R$\n",
    "* Goal: **minimize** $I\\!E [ L(f,Z) ]$ under unknown generating process $P(Z)$\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "* examples are (input, target) pairs: $Z=(X,Y)$ and $f$ takes $X$ as parameter\n",
    "* Most common types:\n",
    "    * **regression**: $Y\\in I\\!R$ scalar or vector\n",
    "        * output of $f$ is in the same set of values as $Y$\n",
    "        * often, **squared error** function is taken as loss function: $$L(f, (X,Y))=\\vert\\vert \\ f(X)-Y \\ \\vert\\vert^2$$\n",
    "    * **classification**: $Y \\in I\\!Z$ (finite)\n",
    "        * often-used loss function: **negative conditional log-likelihood**: $f_i(X)$ estimates \n",
    "$$\\Pr(Y=i\\ \\vert \\ X): \\\\L(f,(X,Y)) = -\\log f_Y(X) \\ \\ , f_Y(X)\\geq 0, \\sum_i f_i(X)=1$$\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "* learn function $f$ to characterize the unknown distribution $P(Z)$\n",
    "* Clustering\n",
    "    * Hard clustering (e.g. k-means)\n",
    "    * Soft clustering (Gaussian mixture models)\n",
    "* Construct a new representation for $Z$ (e.g. PCA)\n",
    "\n",
    "### Local Generalization\n",
    "\n",
    "* Assumption: if input $x_i$ is close to $x_j$, than output $f(x_i)$ and $f(x_j)$ should also be close to each other\n",
    "    * Limitations: **curse of dimensionality**\n",
    "    \n",
    "### Distributed vs. Local Representation and Non-Local Generalization\n",
    "\n",
    "**Local representation**: integer $N$ is represented by sequence of $B$ bits such that $N<B$ and all bits are $0$ except the $N$-th one.\n",
    "\n",
    "**Distributed representation**: integer $N$ is represented by a sequence of $\\log_2B$ bits with usual encoding.\n",
    "\n",
    "$\\Rightarrow$ distributed representation is exponentially more efficient than local one\n",
    "\n",
    "#### Other example:\n",
    "**Clustering** vs **PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Algorithms\n",
    "\n",
    "**Flow graph**: Graph representing a computation, in which each node represents an elementary computation and a value.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "Given expression $\\sin(a^2+b/a) \\Rightarrow$\n",
    "* Graph with two input nodes $a$ and $b$\n",
    "* one node for the division $b/a$\n",
    "* one node for the square (taking only $a$)\n",
    "* one node for the addition ($a^2+b/a$)\n",
    "* one output node computing sine\n",
    "\n",
    "The **depth** of the graph is the length of the longest path from an input to an output\n",
    "\n",
    "### Motivation\n",
    "\n",
    "* In shallow architectures, the number of nodes and parameters grow very large (e.g. logical gates)\n",
    "* The brain has a deep architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
