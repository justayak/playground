{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "(http://deeplearning.net/tutorial/)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Formalization of Learning\n",
    "\n",
    "Given:\n",
    "* training examples $D=\\{z_1, z_2, ..., z_n \\}$ where $z_i$ is sampled from an **unkown** process $P(Z)$\n",
    "* loss function $L(f,Z)$ with arguments:\n",
    "    * decision function $f$\n",
    "    * example $z$\n",
    "    * return scalar $\\in I\\!R$\n",
    "* Goal: **minimize** $I\\!E [ L(f,Z) ]$ under unknown generating process $P(Z)$\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "* examples are (input, target) pairs: $Z=(X,Y)$ and $f$ takes $X$ as parameter\n",
    "* Most common types:\n",
    "    * **regression**: $Y\\in I\\!R$ scalar or vector\n",
    "        * output of $f$ is in the same set of values as $Y$\n",
    "        * often, **squared error** function is taken as loss function: $$L(f, (X,Y))=\\vert\\vert \\ f(X)-Y \\ \\vert\\vert^2$$\n",
    "    * **classification**: $Y \\in I\\!Z$ (finite)\n",
    "        * often-used loss function: **negative conditional log-likelihood**: $f_i(X)$ estimates \n",
    "$$\\Pr(Y=i\\ \\vert \\ X): \\\\L(f,(X,Y)) = -\\log f_Y(X) \\ \\ , f_Y(X)\\geq 0, \\sum_i f_i(X)=1$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
