{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "(https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html)\n",
    "\n",
    "## 1. The Problem\n",
    "\n",
    "### Elements of Reinforcement Learning\n",
    "\n",
    "Beyond the agent and the environment, one can identify four main subelements for a reinforcement learning system:\n",
    "\n",
    "* **a policy**: (stimulus-response rule) defines the learning agents way of behaving at a given time. Map from perceived states of the environment to actions to be taken when in those states.\n",
    "* **a reward function**: defines the goal in a RL problem. Maps each perceived state (or state/action pair) of the environment to a single number, indicating the desirability of that state. A RL agents sole objective is to maximize the total reward it receives in the long run. This function must be unalterable by the agent! Reward functions may be stochastic. Reward functions indicate what is good in an immediate sense as they are basically given directly by the environment.\n",
    "* **a value function**: specifies what is good in the long run. The value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state. Values are predicitions of rewards and must be estimated and reestimated from the sequences of observations an agent makes.\n",
    "* **a model of the environment (optional)**: mimics the behavior of the environment. E.g., given a state and action, the model might predict the resultant next state and next reward. Models are used for *planning*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluative Feedback\n",
    "\n",
    "### n-armed Bandit\n",
    "\n",
    "**Learning problem**:repeated choice among $n$ different actions, after each choice, a numerical reward is received, chosen from a stationary probability distribution, depending on the selected action.\n",
    "\n",
    "**Objective**: Maximize the expected total reward over some time (e.g. 1000 actions) -> each action selection is called a *play*.\n",
    "\n",
    "The **value** of the action is the *mean* of each action which is unknown to the agent at the beginning. When *exploring*, the agent choses a random action and when *exploiting*, the agent is chosing greedily the action with best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def h1():\n",
    "    return random(-2,2)\n",
    "def h2():\n",
    "    return random(0,2)\n",
    "def h3():\n",
    "    return random(-1,4)\n",
    "bandit = [h1, h2, h3]\n",
    "\n",
    "n = len(bandit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-Value Methods\n",
    "\n",
    "The true value of action $a$ is denoted as $Q^*(a) = I\\!E(a)$ and the estimated vaue at the $t$th play as $Q_t(a)$.\n",
    "\n",
    "$$Q_t(a)=\\frac{r_1+r_2+...+r_{k_a}}{k_a}$$\n",
    "\n",
    "If $k_a = 0$, we define $Q_t(a)$ to be some default value. As $k_a \\rightarrow \\infty$, $Q_t(a)$ converges to $Q^*(a)$ (**law of large numbers**).\n",
    "\n",
    "**Simple action selection rule**: On play $t$ greedily select action $a^*$ for which $Q_t(a^*)=\\max_aQ_t(a)$. This method always exploits current knowledge to maximize immediate reward.\n",
    "\n",
    "**$\\epsilon$-greedy method**: Do as the simple action selection rule above, but, every once in a while, with smal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from random import randint, random"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
