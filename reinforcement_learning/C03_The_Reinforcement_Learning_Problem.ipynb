{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Reinforcement Learning Problem\n",
    "(https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html)\n",
    "\n",
    "## The Agent-Environment Interface\n",
    "\n",
    "* Agent and environent interact in discrete time steps $t=0,1,2,3...$\n",
    "* At each time step $t$ the agent receives some representation of the environments *state*, $S_t\\in S$ where $S$ is the set of possible states\n",
    "* .. and on that basis selects an *action*, $A_t\\in A(S_t)$ where $A(S_t)$ is the set of actions available to state $S_t$.\n",
    "* At each step a **policy** $\\pi_t$ is evaluated, where $\\pi_t(a\\vert s)$ is the $\\Pr$ that $A_t=a$ if $S_t=s$\n",
    "\n",
    "\n",
    "## Goals and Rewards\n",
    "\n",
    "* Reward is at each step is a simple number $R_t\\in I\\!R$\n",
    "* Must be comuted in the environment and not in the agent!\n",
    "\n",
    "\n",
    "## Returns\n",
    "\n",
    "Sequence of rewards received after each time step $t$ is denoted $R_{t+1}, R_{t+2}, R_{t+2},...$ $\\Rightarrow$ Seek to **maximize the expected return** $G_t$. In the simplest case the return is the sum of rewards:\n",
    "\n",
    "$$G_t = R_{t+1}+R_{t+2}+...+R_T$$\n",
    "\n",
    "where $T$ is the final time step (**episodic tasks**).\n",
    "\n",
    "**Continual process-control**: Agent-environment interaction goes on continually without limit.\n",
    "**Problem**: $T\\rightarrow \\infty$, therefor we need **discounting** whith a *discount rate* $0\\leq \\gamma \\leq 1$:\n",
    "\n",
    "$$G_t=R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+...=\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}$$\n",
    "\n",
    "* $\\gamma<1$: infinite sum  has a finite value as long as the reward sequence is bounded\n",
    "* $\\gamma=0$ **myopic**, agent is only concerned with immediate reward\n",
    "\n",
    "$$G_t=\\sum_{k=0}^{T-t-1}\\gamma^kR_{t+k+1}$$\n",
    "\n",
    "## Markov Decision Process\n",
    "\n",
    "finite MDP, given any state $s$ and action $a$, the **transition probabilities** of each possible next state $s'$ are:\n",
    "\n",
    "$$p(s'\\ \\vert \\ s,a)=\\Pr\\{S_{t_1}=s' \\ \\vert \\ S_t=s,A_t=a\\}$$\n",
    "\n",
    "The **expected value** of next reward:\n",
    "\n",
    "$$r(s,a,s')=I\\!E\\biggr[ R_{t+1} \\ \\vert \\ S_t=s,A_t=a,S_{t+1}=s'\\biggr]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Functions\n",
    "\n",
    "A policy $\\pi$ is a mapping from each state $s\\in S$ and action $a \\in A(s)$ to the probability $\\pi(a\\ \\vert \\ s)$ of taking action $a$ when in state $s$.\n",
    "\n",
    "Estimate, *how good* it is for the agent to be in a given state.\n",
    "**State-value function** for policy $\\pi$:\n",
    "\n",
    "$$v_\\pi(s)=I\\!E_\\pi \\biggr[ G_t \\ \\biggr\\vert \\ S_t=s \\biggr] = I\\!E_\\pi \\biggr[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\ \\biggr\\vert \\ S_t=s \\biggr]$$\n",
    "\n",
    "\n",
    "$$v_\\pi(s)=I\\!E\\biggr[ G_t \\vert S_t=s \\biggr]\\\\\n",
    "=\\sum_a\\pi(a\\ \\vert \\ s)\\sum_{s'}p(s'\\ \\vert \\ s,a)\\biggr[r(s,a,s')+\\gamma v_\\pi(s')\\biggr]$$\n",
    "\n",
    "which is the **Bellman equation** for $v_{\\pi}$. It expresses a relationship between the value of a state and the values of its successor states.\n",
    "\n",
    "**Action-value function** for policy $\\pi$:\n",
    "\n",
    "$$q_\\pi(s,a)=I\\!E_\\pi \\biggr[ G_t \\vert S_t=s,A_t=a\\biggr] =\n",
    "I\\!E_\\pi\\biggr[ \\sum_{k=0}^\\infty y^k R_{t+k+1} \\biggr\\vert S_t=s, A_t=a \\biggr]$$\n",
    "\n",
    "\n",
    "### Optimal Value Functions\n",
    "\n",
    "A policy $\\pi$ is defined to be **better** than or equal to a policy $\\pi'$ if $v_{\\pi}(s) \\geq v_{\\pi'}(s) \\forall s\\in S$\n",
    "\n",
    "*optimal state-value function*:\n",
    "$$v_*(s)=\\max_\\pi v_\\pi(s)\\\\\n",
    "=\\max_a I\\!E_{\\pi^*}\\biggr[ G_t \\ \\vert \\ S_t=s, A_t =a \\biggr]\\\\\n",
    "=\\max_{a\\in A(s)} \\sum_{s'} p(s' \\ \\vert \\ s, a) \\biggr[ r(s,a,s') + \\gamma v_*(s') \\biggr]$$\n",
    "\n",
    "$$q_*(s,a)=\\max_\\pi q_\\pi(s,a) \\\\\n",
    "=I\\!E\\biggr[R_{t+1} + \\gamma v_*(S_{t+1} \\ \\vert \\ S_t=s, A_t=a \\biggr]\\\\\n",
    "=\\sum_{s'}p(s' \\ \\vert \\ s, a) \\biggr[ r(s,a,s')+\\gamma \\max_{a'}q_*(s',a')\\biggr]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example: Grid-world\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
