{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "(https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html)\n",
    "\n",
    "## 1. The Problem\n",
    "\n",
    "### Elements of Reinforcement Learning\n",
    "\n",
    "Beyond the agent and the environment, one can identify four main subelements for a reinforcement learning system:\n",
    "\n",
    "* **a policy**: (stimulus-response rule) defines the learning agents way of behaving at a given time. Map from perceived states of the environment to actions to be taken when in those states.\n",
    "* **a reward function**: defines the goal in a RL problem. Maps each perceived state (or state/action pair) of the environment to a single number, indicating the desirability of that state. A RL agents sole objective is to maximize the total reward it receives in the long run. This function must be unalterable by the agent! Reward functions may be stochastic. Reward functions indicate what is good in an immediate sense as they are basically given directly by the environment.\n",
    "* **a value function**: specifies what is good in the long run. The value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state. Values are predicitions of rewards and must be estimated and reestimated from the sequences of observations an agent makes.\n",
    "* **a model of the environment (optional)**: mimics the behavior of the environment. E.g., given a state and action, the model might predict the resultant next state and next reward. Models are used for *planning*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluative Feedback\n",
    "\n",
    "### n-armed Bandit\n",
    "\n",
    "**Learning problem**:repeated choice among $n$ different actions, after each choice, a numerical reward is received, chosen from a stationary probability distribution, depending on the selected action.\n",
    "\n",
    "**Objective**: Maximize the expected total reward over some time (e.g. 1000 actions) -> each action selection is called a *play*.\n",
    "\n",
    "The **value** of the action is the *mean* of each action which is unknown to the agent at the beginning. When *exploring*, the agent choses a random action and when *exploiting*, the agent is chosing greedily the action with best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def h1():\n",
    "    return random(-2,2)\n",
    "def h2():\n",
    "    return random(0,2)\n",
    "def h3():\n",
    "    return random(-1,4)\n",
    "bandit = [h1, h2, h3]\n",
    "\n",
    "n = len(bandit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-Value Methods\n",
    "\n",
    "The true value of action $a$ is denoted as $Q^*(a) = I\\!E(a)$ and the estimated vaue at the $t$th play as $Q_t(a)$.\n",
    "\n",
    "$$Q_t(a)=\\frac{r_1+r_2+...+r_{k_a}}{k_a}$$\n",
    "\n",
    "If $k_a = 0$, we define $Q_t(a)$ to be some default value. As $k_a \\rightarrow \\infty$, $Q_t(a)$ converges to $Q^*(a)$ (**law of large numbers**).\n",
    "\n",
    "**Simple action selection rule**: On play $t$ greedily select action $a^*$ for which $Q_t(a^*)=\\max_aQ_t(a)$. This method always exploits current knowledge to maximize immediate reward.\n",
    "\n",
    "**$\\epsilon$-greedy method**: Do as the simple action selection rule above, but, every once in a while, with small probability $\\epsilon$, seelect an action at random uniformly.\n",
    "\n",
    "### Softmax Action Selection\n",
    "\n",
    "$\\epsilon$-greedy action selection is effective to balance exploration and exploitation. The drawback is that it chooses equally among all actions. So it is equally likely to chose the worst or the best action which might be an issue, if the *worst* action might be really bad.\n",
    "\n",
    "**Solution**: vary action probabilities as a graded function of estimated value. Greedy action has the highest probability, but all others are ranked and weighted according to their value estimates.\n",
    "\n",
    "$$\\Pr_t^{Softmax}(a)=\\frac{e^{\\frac{Q_t(b)}{\\tau}}}{\\sum_{b=1}^{n}e^{\\frac{Q_t(a)}{\\tau}}}$$\n",
    "\n",
    "$\\tau$ is a positive parameter called *temperatur*.\n",
    "\n",
    "* High temperatures cause actions to be nearly equiprobable\n",
    "* Low temperatures produce greater difference in selection probability\n",
    "\n",
    "When $\\tau \\rightarrow 0$, softmax action selection becomes the same as greedy action selection.\n",
    "\n",
    "### Evaluation vs Instruction\n",
    "\n",
    "Example: Suppose there are $100$ possible actions and action $32$ is selected. **Evaluative** feedback would give a score, say $7.2$ while **instructive** training would say what other action, e.g. $67$ was correct instead.\n",
    "\n",
    "#### $L_{R-P}$ algorithm\n",
    "\n",
    "**Linear reward penalty**: $\\pi_t(a)$ is the probability, that action $a$ is chosen.\n",
    "\n",
    "$$\\pi_{t+1}(a)=\\pi_t(d_t)+\\alpha\\biggr[1-\\pi_t(d_t)\\biggr]$$\n",
    "\n",
    "### Incremental Implementation\n",
    "\n",
    "$$Q_t(a)=\\frac{r_1+r_2+...+r_{k_a}}{k_a}$$\n",
    "\n",
    "where $r_1, ..., r_{k_a} are all the rwards received following all selections of action $a$ prior to play $t$.\n",
    "\n",
    "**Incremental update** to sensibly implement the function on a computer:\n",
    "$Q_k$ denotes the average of its first $k$ rewards for some action $a$ (but: $Q_k \\neq Q_k(a)$). Given this average and a $(k+1)$st reward, the $r_{k+1}$, then the average of all $k+1$ rewards can be computed by:\n",
    "\n",
    "\n",
    "$$Q_{k+1}= \\frac{1}{k+1}\\sum_{i=1}^{k+1}r_i\\\n",
    "=Q_k + \\frac{1}{k+1} \\biggr[r_{k+1}-Q_k\\biggr]$$\n",
    "\n",
    "**General update rule**:\n",
    "$$NewEstimate \\leftarrow OldEstimate + StepSize \\biggr[Target - OldEstimate \\biggr]$$\n",
    "\n",
    "$\\biggr[Target - OldEstimate\\biggr]$ is an *error* in the estimate which is reduced by taking a step towards the *Target*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking a Nonstationary Problem\n",
    "\n",
    "In non-stationary problems, it make sense to weight recent rewards stronger than long-past ones. One solution is the use of an\n",
    "**constant step size** parameter $0 < \\alpha \\leq 1$. \n",
    "\n",
    "$$Q_{K+1}=Q_k + \\alpha \\biggr[r_{k+1}-Q_k\\biggr]$$\n",
    "\n",
    "$Q_0$ is the initial estimate.\n",
    "\n",
    "$$Q_k=Q_{k-1} + \\alpha \\biggr[r_k - Q_{k-1}\\biggr]\\\\\n",
    "=(1-\\alpha)^kQ_0 + \\sum_{i=1}^{k} \\alpha (1-\\alpha)^{k-i} r_i$$\n",
    "\n",
    "Sum of the weigts is $(1-\\alpha)^k + \\sum_{i=1}^{k} \\alpha (1-\\alpha)^{k-i}=1$\n",
    "\n",
    "### Reinforcement comparison\n",
    "\n",
    "Create a *reference reward* $\\overline{r_t}$ which is the average of all rewards to be able to tell apart good from bad results. These methods are sometimes more efficient than **action-value** methods.\n",
    "\n",
    "$$\\pi_t(a)=\\Pr\\{ a_t = a\\} = \\frac{e^{p_t(a)}}{\\sum_{b=1}^{n}e^{p_t(b)}}$$\n",
    "\n",
    "$$p_{t+1}(a_t)=p_t(a_t)+\\beta \\biggr[r_t-\\overline{r_t}\\biggr]$$\n",
    "\n",
    "$\\beta$ is a positive step-size parameter that implements the idea that high rewards should increase the probability and low rewards should decrease it.\n",
    "\n",
    "$$\\overline{r_{t+1}}=\\overline{r_t}+\\alpha\\biggr[r_t-\\overline{r_t}\\biggr], 0 < \\alpha \\leq 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from random import randint, random"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
