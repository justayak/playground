{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "by Bishop\n",
    "\n",
    "## Feed-forward Network functions\n",
    "\n",
    "* 2-layer network with one hidden layer, assuming the bias is added to $\\mathbf{w}$ and assuming $x \\leftarrow \\big[ \\ 1 \\ \\ x^T \\ \\big]^T$:\n",
    "\n",
    "$$y_k( \\mathbf{x}, \\mathbf{w}) = \\sigma \\biggr( \n",
    "\\sum_{j=1}^M w_{kj}^{(2)} h \\big(\n",
    "\\sum_{i=1}^D w_{ji}^{(1)} x_i\n",
    "\\big)\n",
    "\\biggr) $$\n",
    "\n",
    "* $h(\\circ)$ is nonlinear function (usually sigmoidal)\n",
    "* $\\gamma(\\circ)$ determined by nature of data: e.g.:\n",
    "    * **standard regression**: identity, so that: $y_k = a_k$\n",
    "    * **multiple binary classification**: logistic sigmoid s.t. $\\sigma(a) = \\frac{1}{1+ \\mathrm{exp}(-a)}$\n",
    "    \n",
    "### Training\n",
    "\n",
    "Cost function:\n",
    "$$E(\\mathbf{w})=\\frac{1}{2}\\sum_{n=1}^N \\ \n",
    "\\vert \\vert \\ y(\\mathbf{x}_n, \\mathbf{w}) - \\mathbf{t}_n \\ \\vert\\vert ^2$$\n",
    "\n",
    "Assume $t$ has a Gaussian distribution with an $x$ dependent mean:\n",
    "\n",
    "$$\n",
    "p(t \\ \\vert \\mathbf{x},\\mathbf{w})=\\mathrm{Norm}\\big(\n",
    "t \\vert \\ y(\\mathbf{x},\\mathbf{w}), \\beta^{-1}\n",
    "\\big)\n",
    "$$\n",
    "\n",
    "Given $\\mathbf{X}=\\{x_1, ..., x_N\\}$ along with target values $\\mathbf{t}=\\{t_1, ..., t_N \\}$ construct likelihood function:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\beta) = \\prod_{n=1}^N\n",
    "p(t_n | \\mathbf{x}_n, \\mathbf{w}, \\beta)\n",
    "$$\n",
    "\n",
    "Taking negative log, obtain new error function which can be used to learn $\\mathbf{w}$ and $\\beta$:\n",
    "\n",
    "$$\n",
    "\\frac{\\beta}{2} \\sum_{n=1}^N \\big(\n",
    "y(\\mathbf{x}_n, \\mathbf{w}) - t_n\n",
    "\\big)^2\n",
    "- \\frac{N}{2} \\ln \\beta + \\frac{N}{2} \\ln(2 \\pi)\n",
    "$$\n",
    "\n",
    "Maximizing the likelihood function is equivalent to minimizing the sum-of-squares error function given by:\n",
    "\n",
    "$$\n",
    "E(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N\\biggr(\n",
    "y(\\mathbf{x}_n, \\mathbf{w}) - t_n\n",
    "\\biggr)^2\n",
    "$$\n",
    "\n",
    "Denote $\\mathbf{w}_{\\mathrm{ML}}$ to be the value of $\\mathbf{w}$ found by minimizing $E(\\mathbf{w})$. Having found $\\mathbf{w}_{\\mathrm{ML}}$, the value of $\\beta$ can be found by minizming negative log likelihood to give:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\beta_{ML}} = \\frac{1}{N}\n",
    "\\sum_{n=1}^N \\biggr(\n",
    "y(\\mathbf{x}_n, \\mathbf{w}_{\\mathrm{ML}}) - t_n\n",
    "\\biggr)^2\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
